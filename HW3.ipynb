{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNLcQGmq+IWOJfelASCAv1X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erikellerx/Projects_in_MLAI/blob/main/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "X4NUhiPJvVpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Research of Framework**\n",
        "\n",
        "I first went through some documentation of PyTorch and some example of how to create a NN using Pytorch. Since I am using google colab, there is no need for me to install PyTorch and figure out the environment setup. However, I still leave a link below about installation.  \n",
        "\n",
        "Link: https://pytorch.org/docs/stable/index.html  https://pytorch.org/ https://www.kaggle.com/code/kanncaa1/pytorch-tutorial-for-deep-learning-lovers/notebook\n",
        "\n",
        "\n",
        "Since I want to use MNIST dataset for this project, I looked though some basic of MNIST and how to load it. I could, therefore, know how much of a performance I could achieve using 2 layer NN. \n",
        "\n",
        "Link: http://yann.lecun.com/exdb/mnist/ https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST\n",
        "\n",
        "Overall, I need torch.nn to create my NN network structure and loss function, torchvision.datasets to load my MNIST dataset, torch.optim to define my optimizer, torchvision.transforms.ToTensor to transfer my data into tensor. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tlu7fg-ex5Z0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9PIg5IvrcKD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "torch.manual_seed(9)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**load and split dataset**"
      ],
      "metadata": {
        "id": "JYhnKkmm6kr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = True,                         \n",
        "    transform = ToTensor(), \n",
        "    download = True,            \n",
        ")\n",
        "test_data = datasets.MNIST(\n",
        "    root = 'data', \n",
        "    train = False, \n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "val_data, test_data = torch.utils.data.random_split(test_data, [5000,5000])"
      ],
      "metadata": {
        "id": "f5t9J3PssPNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(ncols=10, sharex=False, \n",
        "    sharey=True, figsize=(10, 4))\n",
        "for i in range(10):\n",
        "    x, y = train_data[i]\n",
        "    axes[i].set_title(y)\n",
        "    axes[i].imshow(x.view(28,28), cmap='gray')\n",
        "    axes[i].get_xaxis().set_visible(False)\n",
        "    axes[i].get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AICvSPyYHba0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [ each[1] for each in train_data]\n",
        "sns.countplot(x=labels, color=\"purple\")\n",
        "plt.title(\"Distribution of Digits\")\n",
        "plt.xlabel(\"Image Label\")\n",
        "plt.ylabel(\"Count\")"
      ],
      "metadata": {
        "id": "hJO0j4yZKrFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create NN and Train\n"
      ],
      "metadata": {
        "id": "Solu0Xf36Tgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters**"
      ],
      "metadata": {
        "id": "Fn4GoRk26np-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 10\n",
        "batch = 64 \n",
        "lr = 0.01 \n",
        "momentum = 0.9"
      ],
      "metadata": {
        "id": "pJHIi00iXcP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch, shuffle = True)\n",
        "val_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size = 1000, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = 1000, shuffle = True)"
      ],
      "metadata": {
        "id": "eLQbhuIsj_Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**define out neural network**"
      ],
      "metadata": {
        "id": "yv1nYqON6Z0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.linear1 = nn.Linear(28 * 28, 256)\n",
        "    self.linear2 = nn.Linear(256, 10)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.linear2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "MeiRMwkwqipu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr = lr, momentum = momentum)\n",
        "criterion  = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "NLl8jypJVrqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochlist = [e for e in range(epoch)]\n",
        "trainLoss = []\n",
        "trainAcc = []\n",
        "valLoss = []\n",
        "valAcc = []\n",
        "\n",
        "\n",
        "for e in range(epoch):\n",
        "  #Train\n",
        "  \n",
        "  correct = 0\n",
        "  totalLoss = 0\n",
        "  model.train()\n",
        "  for i, (x, y) in enumerate(train_loader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_pred = model(x.view(-1, 28*28))\n",
        "    loss = criterion(y_pred, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    label = y_pred.argmax(dim=1, keepdim=True)\n",
        "    correct += label.eq(y.view_as(label)).sum().item()\n",
        "    totalLoss += loss.item()\n",
        "\n",
        "    if i % 50 == 0:\n",
        "      print(f\"Training...epoch: {e}, loss: {loss.item()}\")\n",
        "  trainAcc.append(correct / len(train_loader.dataset))\n",
        "  trainLoss.append(totalLoss / len(train_loader))\n",
        "  \n",
        "\n",
        "  #Validation\n",
        "  correct = 0\n",
        "  loss = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for i, (x, y) in enumerate(val_loader):\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      y_pred = model(x.view(-1,28*28))\n",
        "      loss += criterion(y_pred, y).item()\n",
        "      label = y_pred.argmax(dim=1, keepdim=True)\n",
        "      correct += label.eq(y.view_as(label)).sum().item()\n",
        "\n",
        "  loss /= len(val_loader)\n",
        "  acc = correct / len(val_loader.dataset)\n",
        "  print(f\"Validation...epoch: {e}, loss : {loss}, acc: {acc}\")\n",
        "  valAcc.append(acc)\n",
        "  valLoss.append(loss)\n"
      ],
      "metadata": {
        "id": "ACmeYHoDl9NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Loss\")\n",
        "plt.plot(epochlist, trainLoss, label = \"train\" )\n",
        "plt.plot(epochlist, valLoss, label = \"Val\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.title(\"Accuracy\")\n",
        "plt.plot(epochlist, trainAcc, label = \"Acc\" )\n",
        "plt.plot(epochlist, valAcc, label = \"Val\" )\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bZeepqLNfxkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "model.eval()\n",
        "for x, y in test_loader:\n",
        "  x,y = x.to(device), y.to(device)\n",
        "  y_pred = model(x.view(-1,28 * 28))\n",
        "  label = y_pred.argmax(dim = 1)\n",
        "  correct += label.eq(y.view_as(label)).sum().item()\n",
        "\n",
        "acc = correct / len(test_loader.dataset)\n",
        "print(f\"Testing Accuracy: {acc}\")"
      ],
      "metadata": {
        "id": "T7BgacNjcR9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters**\n",
        "\n",
        "**epoch**: I initialize set epoch to 10, which doesn't show a sign of overfitting bese on the graph above. However, I also tried 20 and there is clearly overfitting start epoch 15 and so. The sweet spot of epoch should be around 8~12. \n",
        "\n",
        "**Batch**: I set batch to 64. I initially set my batch to a larger number, which lead a very poor training accuracy. I think this is because our gradient had stucked into a local minimum. \n",
        "\n",
        "**lr (Learning rate)**: I initialize set my lr to 0.001. However, the accuracy within 10 epoch could surpass 90%. This is a sign of underfitting. Therefore, I raise my lr to 0.01 and my model converge fairly quickly and result to a high testing and validation accuracy.\n",
        "\n",
        "**Momentum**: I didn't change much in terms of momentum. I set this one to 0.9 as suggested. \n",
        "\n",
        "I didn't use any form of regularization. This is because my model doesn't show a quick overfitting and, therefore, using regularization isn't optimal. \n",
        "\n",
        "There are two hidden layer in my NN and both of them are nn.Linear. I used ReLU in between to keep this simple. In my two hidden layer, I down scale my 784 features into 256, then to 10. The final 10 will correspond my targeted output. I used SDG + momentum as my optimizer. I also tried Adam, Adagrad, and Adadelta. However, I think SDG is good enough for this task, whereas other three optimizers mention above could lead to a faster converging (faster overfitting of course). \n"
      ],
      "metadata": {
        "id": "g5P-n6Jt4xK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "snTINlyVk4LL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree \n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "Kt8KPXzFlrE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.MNIST('./data', train=True, download=True).data.numpy()\n",
        "test_data = datasets.MNIST('./data', train=False, download=True).data.numpy()\n",
        "\n",
        "train = []\n",
        "for each in train_data:\n",
        "  train.append(each.flatten())\n",
        "\n",
        "test = []\n",
        "for each in test_data:\n",
        "  test.append(each.flatten())\n",
        "\n",
        "train_data = np.array(train).astype(float)\n",
        "test_data = np.array(test).astype(float)\n",
        "\n",
        "train_target = datasets.MNIST('./data', train=True, download=True).targets.numpy()\n",
        "test_target = datasets.MNIST('./data', train=False, download=True).targets.numpy()\n",
        "\n",
        "model = tree.DecisionTreeClassifier()\n",
        "model = model.fit(train_data, train_target)\n",
        "\n",
        "pred = model.predict(test_data)\n",
        "dtacc = accuracy_score(pred, test_target)\n",
        "\n",
        "print(f\"Decision Tree Acc: {dtacc}\")"
      ],
      "metadata": {
        "id": "pBsS9pRFk3vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize =(6, 3))\n",
        "\n",
        "model = (\"Decision Tree\", \"Neural Network\")\n",
        "score = (dtacc, acc)\n",
        "ax.barh(model, score)\n",
        "\n",
        "# Remove axes splines\n",
        "for s in ['top', 'bottom', 'left', 'right']:\n",
        "    ax.spines[s].set_visible(False)\n",
        "\n",
        "for i in ax.patches:\n",
        "    plt.text(i.get_width()+0.2, i.get_y()+0.5,\n",
        "             str(round((i.get_width()), 2)),\n",
        "             fontsize = 10, fontweight ='bold',\n",
        "             color ='grey')\n"
      ],
      "metadata": {
        "id": "nFcXCPqL1t5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result comparison shows a clearly sign of the fact that NN is outperform Decision Tree in MNIST dataset. The accuracy difference is about 10% in my implementation. The reason why NN perform so good is that NN could capture better pattern of 10 class classification. Decision Tree, however, could only draw straight lines in feature space. Therefore, couldn't capture such variance of pattern and difference between each class.  "
      ],
      "metadata": {
        "id": "Aq-5C01fbvJH"
      }
    }
  ]
}